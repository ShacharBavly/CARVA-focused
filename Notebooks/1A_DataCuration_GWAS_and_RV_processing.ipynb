{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Curation Notebook A - Processing of GWAS and RV Data\n",
    "\n",
    "This notebook performs the inital processing of common and rare variant association data:\n",
    "\n",
    "- Filtering CV and RV studies\n",
    "- Filtering gene associations\n",
    "- Converting to NCBI Gene IDs\n",
    "- Cleaning the trait EFO codes\n",
    "\n",
    "**Inputs:**\n",
    "* GWAS Study Information (`gwas-catalog-v1.0.3.1-studies-r2025-03-26.tsv.gz`)\n",
    "* GWAS Associations (`gwas_catalog_Jan29_2025.txt.gz`)\n",
    "* GWAS Trait EFO Mappings (`gwas_catalog_trait-mappings_r2025-03-26.tsv.gz`)\n",
    "* RV Study Information (`rv_study_info_cleaned_with_manual.tsv`)\n",
    "* RV Associations (`rv_study_info_cleaned_with_manual_mapped_Mar28.tsv`)\n",
    "\n",
    "**Figures generated in this notebook:**\n",
    "- None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T19:13:40.757408Z",
     "start_time": "2025-10-03T19:13:36.823801Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from neteval import gene_mapper as gm\n",
    "from neteval import query_ensembl as qe\n",
    "from neteval import query_hgnc as qh\n",
    "import re\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import sentence_transformers\n",
    "model = sentence_transformers.SentenceTransformer('paraphrase-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T19:14:36.730705Z",
     "start_time": "2025-10-03T19:14:36.728974Z"
    }
   },
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "datadir = os.path.join(cwd, '..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GWAS Catalog - Common Variant Associations\n",
    "\n",
    "GWAS catalog study information can be downloaded from: https://www.ebi.ac.uk/gwas/docs/file-downloads (All studies v1.0.3.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter Studies\n",
    "\n",
    "GWAS studies are filtered to:\n",
    "* Remove studies with background traits\n",
    "* Keep only genome-wide studies\n",
    "* Exclude studies with missing trait information, or multiple trait mappings\n",
    "* Exclude studies with too few associations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T19:14:38.105829Z",
     "start_time": "2025-10-03T19:14:36.751294Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Studies: 138810\n",
      "- Studies with background trait: 124463\n",
      "- Not genome-wide studies: 101460\n",
      "- Studies with missing trait information: 101402\n",
      "- Studies with multiple traits: 95859\n",
      "- Studies with at < 3 associations: 19452\n",
      "Total filtered studies: 19452\n"
     ]
    }
   ],
   "source": [
    "# Get study information\n",
    "study_info = pd.read_csv(os.path.join(datadir,'Reference_Data', 'gwas-catalog-v1.0.3.1-studies-r2025-03-26.tsv.gz'), \n",
    "                         sep='\\t', low_memory=False)\n",
    "print(f'Initial Studies:', len(study_info))\n",
    "\n",
    "# Remove studies with background traits\n",
    "study_info = study_info[study_info['MAPPED BACKGROUND TRAIT'].isna()]\n",
    "print(f'- Studies with background trait:', len(study_info))\n",
    "\n",
    "# Use only genome-wide studies\n",
    "study_info['GENOTYPING TECHNOLOGY'] = study_info['GENOTYPING TECHNOLOGY'].apply(lambda x: x.split('[')[0].strip())\n",
    "study_info = study_info[study_info['GENOTYPING TECHNOLOGY'].isin(['Genome-wide genotyping array', \n",
    "       'Genome-wide genotyping array, Genome-wide sequencing',])]\n",
    "print(f'- Not genome-wide studies:', len(study_info))\n",
    "\n",
    "# Remove studies with missing trait information\n",
    "study_info = study_info.dropna(subset=['DISEASE/TRAIT', 'MAPPED_TRAIT', 'MAPPED_TRAIT_URI'])\n",
    "print(f'- Studies with missing trait information:', len(study_info))\n",
    "\n",
    "# Remove studies mapped to multiple traits\n",
    "study_info = study_info[~study_info['MAPPED_TRAIT_URI'].str.contains(', ')]\n",
    "print(f'- Studies with multiple traits:', len(study_info))\n",
    "\n",
    "# Keep studies with at least 3 associations (will be filtered based on genes later)\n",
    "study_info = study_info[study_info['ASSOCIATION COUNT'] >= 3]\n",
    "print(f'- Studies with at < 3 associations:', len(study_info))\n",
    "print(f'Total filtered studies:', len(study_info))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean trait information\n",
    "\n",
    "Mappings between trait descriptions and EFO terms are in the GWAS Catalog are not one-to-one. Therefore, we will take the best matched EFO term for each unique trait description.   \n",
    "\n",
    "Trait mappings can be downloaded from: https://www.ebi.ac.uk/gwas/docs/file-downloads (GWAS to EFO mappings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T19:23:13.830951Z",
     "start_time": "2025-10-03T19:23:13.646380Z"
    }
   },
   "outputs": [],
   "source": [
    "trait_info = pd.read_csv(os.path.join(datadir,'Reference_Data',  'gwas_catalog_trait-mappings_r2025-03-26.tsv.gz'), sep='\\t')\n",
    "\n",
    "# Identify traits present in filtered studies, and the number of occurences of each trait\n",
    "trait_info = trait_info[trait_info['Disease trait'].isin(study_info['DISEASE/TRAIT'])]\n",
    "trait_disease_counts = trait_info['Disease trait'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T19:14:38.482198Z",
     "start_time": "2025-10-03T19:14:38.476754Z"
    }
   },
   "outputs": [],
   "source": [
    "# Identify one-to-one mappings\n",
    "one_match = trait_disease_counts[trait_disease_counts == 1].index\n",
    "one_match = trait_info[trait_info['Disease trait'].isin(one_match)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T19:14:38.987358Z",
     "start_time": "2025-10-03T19:14:38.522622Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initiate dictionary of trait mappings\n",
    "match_dict = {}\n",
    "for i, row in one_match.iterrows():\n",
    "    match_dict[row['Disease trait']] = (row['EFO term'], row['EFO URI'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find best matches for traits with multiple mappings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T19:14:39.051207Z",
     "start_time": "2025-10-03T19:14:39.047374Z"
    }
   },
   "outputs": [],
   "source": [
    "multi_match = trait_disease_counts[trait_disease_counts > 2].index\n",
    "multi_match = trait_info[trait_info['Disease trait'].isin(multi_match)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T19:14:39.122755Z",
     "start_time": "2025-10-03T19:14:39.116691Z"
    },
    "code_folding": [
     1,
     26
    ],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# Sentence embedding cannot always capture acronyms, so need to expand the most common ones\n",
    "def replace_acronyms(text, acronym_dict):\n",
    "\n",
    "    \"\"\"\n",
    "    Replace acronyms in a string with their expansions followed by the original acronym in parentheses.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The input string containing acronyms.\n",
    "        acronym_dict (dict): A dictionary where keys are acronyms (str) and values are the expansions (str).\n",
    "    \n",
    "    Returns:\n",
    "        str: The text with acronyms replaced by their expansion and the original acronym in parentheses.\n",
    "    \"\"\"\n",
    "    # This regex matches words consisting of at least two uppercase letters.\n",
    "    acronym_pattern = re.compile(r'[A-Z]{2,}')\n",
    "    \n",
    "    def replacer(match):\n",
    "        word = match.group(0)\n",
    "        if word in acronym_dict:\n",
    "            # Return the expansion with the original acronym in parentheses.\n",
    "            return f\"{acronym_dict[word]} ({word})\"\n",
    "        else:\n",
    "            return word\n",
    "    \n",
    "    return acronym_pattern.sub(replacer, text)\n",
    "\n",
    "def extract_acronyms(strings):\n",
    "    \"\"\"\n",
    "    Extract acronyms from a list of strings and count their occurrences.\n",
    "    Acronyms are identified as words that consist entirely of uppercase letters.\n",
    "    \n",
    "    Args:\n",
    "        strings (list of str): List of strings to search for acronyms.\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary with acronyms as keys and their counts as values.\n",
    "    \"\"\"\n",
    "    acronym_pattern = re.compile(r'\\b[A-Z]{2,}\\b')\n",
    "    acronym_counts = {}\n",
    "    for text in strings:\n",
    "        found = acronym_pattern.findall(text)\n",
    "        for acronym in found:\n",
    "            acronym_counts[acronym] = acronym_counts.get(acronym, 0) + 1\n",
    "    return acronym_counts\n",
    "\n",
    "def map_trait(trait_str, trait_info, usecol='Disease trait'):\n",
    "    options = trait_info[trait_info[usecol]==trait_str]\n",
    "    # embeddings\n",
    "    trait_embedding = model.encode([trait_str.lower().replace('levels', 'measurement')])\n",
    "    option_embeddings = model.encode([x.lower() for x in options['EFO term'].values])\n",
    "    # cosine similarity\n",
    "    similarities = cosine_similarity(trait_embedding, option_embeddings)\n",
    "    best_option = options.iloc[np.argmax(similarities)]\n",
    "    return (best_option['EFO term'], best_option['EFO URI'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T19:14:39.191216Z",
     "start_time": "2025-10-03T19:14:39.185403Z"
    },
    "code_folding": [
     0,
     17
    ],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# Definitions of common acronyms\n",
    "\n",
    "common_acronyms = {\n",
    "    'LDL': 'low density lipoprotein',\n",
    "    'HDL': 'high density lipoprotein',\n",
    "    'BMI': 'body mass index',\n",
    "    'VLDL': 'very low density lipoprotein',\n",
    "    'IL': 'interleukin',\n",
    "    'IDL': 'intermediate density lipoprotein',\n",
    "    'FVC': 'forced vital capacity',\n",
    "    'FEV': 'forced expiratory volume',\n",
    "    'COVID': 'coronavirus',\n",
    "    'HIV': 'human immunodeficiency virus',\n",
    "    'SARS': 'severe acute respiratory syndrome',\n",
    "    'APOE': 'apolipoprotein E',\n",
    "    'APOB': 'apolipoprotein B',\n",
    "    'APOA': 'apolipoprotein A',\n",
    "}\n",
    "\n",
    "acronym_expansions = {\n",
    "    'ldl': 'low density lipoprotein',\n",
    "    'hdl': 'high density lipoprotein',\n",
    "    \"FAW3\": 'omega-3 polyunsaturated fatty acid',\n",
    "    \"FAW6\": 'omega-6 polyunsaturated fatty acid',\n",
    "    \"FEC\": \"Forced Expiratory volume\",\n",
    "    \"FEV\": \"Forced Expiratory volume\",\n",
    "    \"HIV\": \"Human Immunodeficiency Virus\",\n",
    "    \"IV\": \"4\",\n",
    "    \"LDH\": \"lumbar disc herniation\",\n",
    "    \"NHDL\": 'non High density ipoprotein',\n",
    "    \"NSAID\": 'nonsteroidal anti inflammatory drug',\n",
    "    \"ACACE\": \"Acetoacetate\",                                \n",
    "    \"ACE\": \"Acetate\",             \n",
    "    \"ALA\": \"Alanine\",                       \n",
    "    \"ALB\": \"Albumin\",  \n",
    "    \"APOB\": \"Apolipoprotein B\",  \n",
    "    \"APOC\": \"Apolipoprotein C\",  \n",
    "    \"BMI\": \"Body Mass Index\",   \n",
    "    \"BP\": \"Blood Pressure\",   \n",
    "    \"CHOLA\": \"Cholesterol\",    \n",
    "    \"CIT\": \"Citrate\",      \n",
    "    \"CRP\": \"C-Reactive Protein\",   \n",
    "    \"DHA\": \"Docosahexaenoic Acid\",  \n",
    "    \"DISTRIB\": \"Distribution\",     \n",
    "    \"EGFR\": \"Estimated Glomerular Filtration Rate\",   \n",
    "    \"FRAC\": \"Fraction\",                              \n",
    "    \"FVC\": \"Forced Vital Capacity\",               \n",
    "    \"GLN\": \"Glutamine\",                            \n",
    "    \"GLOL\": \"Glycerol\",                          \n",
    "    \"GLY\": \"Glycine\",                             \n",
    "    \"GP\": \"Glycoproteins\",                        \n",
    "    \"HDL\": \"High Density Lipoprotein\",                  \n",
    "    \"HDLC\": \"High Density Lipoprotein Cholesterol\",     \n",
    "    \"HEEL\": \"Heel Bone Mineral Density\",  \n",
    "    \"HEIGHT\": \"Height\",                         \n",
    "    \"HIS\": \"Histidine\",     \n",
    "    \"IDL\": \"Intermediate Density Lipoprotein\",  \n",
    "    \"IGF\": \"Insulin like Growth Factor\",  \n",
    "    \"III\": \"Type III\",     \n",
    "    \"INS\": \"Insulin\",  \n",
    "    \"LA\": \"Linoleic Acid\",   \n",
    "    \"LDL\": \"Low Density Lipoprotein\",\n",
    "    \"LDLC\": \"Low Density Lipoprotein Cholesterol\",\n",
    "    \"LIGHT\": \"Light Scatter\",\n",
    "    \"MUFA\": \"Monounsaturated Fatty Acids\",\n",
    "    \"PC\": \"Phosphatidylcholine\",          \n",
    "    \"PHE\": \"Phenylalanine\",               \n",
    "    \"PUFA\": \"Polyunsaturated Fatty Acids\",\n",
    "    \"PYR\": \"Pyruvate\",                    \n",
    "    \"RBC\": \"Red Blood Cell Count\",        \n",
    "    \"SCZD\": \"Schizophrenia\",              \n",
    "    \"SHBG\": \"Sex Hormone-Binding Globulin\",\n",
    "    \"SPHERED\": \"Spherical Diameter\",       \n",
    "    \"SYST\": \"Systolic Blood Pressure\",     \n",
    "    \"TOTCHO\": \"Total Cholines\",            \n",
    "    \"TOTCHOL\": \"Total Cholesterol\",        \n",
    "    \"TOTPG\": \"Total Phosphoglycerides\",    \n",
    "    \"TSCORE\": \"T-Score\",                   \n",
    "    \"TYR\": \"Tyrosine\",                     \n",
    "    \"VAL\": \"Valine\",                       \n",
    "    \"VIT\": \"Vitamin (unspecified)\",      \n",
    "    \"VLDL\": \"Very Low Density Lipoprotein\", \n",
    "    \"VLDLPL\": \"Very low density lipoprotein Phospholipids\",  \n",
    "    \"VLDLTG\": \"Very low density lipoprotein Triglycerides\",  \n",
    "    \"VOL\": \"Volume\",            \n",
    "    \"Whr\": \"Waist Hip Ratio\", \n",
    "    \"whr\": \"Waist Hip Ratio\",\n",
    "    \"WHR\": \"Waist Hip Ratio\",            \n",
    "    \"XL\": \"Extra Large\",                 \n",
    "    \"XS\": \"Extra Small\"                  \n",
    "}\n",
    "\n",
    "all_acronyms = {**common_acronyms, **acronym_expansions}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T19:14:39.377593Z",
     "start_time": "2025-10-03T19:14:39.256988Z"
    }
   },
   "outputs": [],
   "source": [
    "# expand the acronymns in both data sets\n",
    "study_info['TraitExp'] =study_info['DISEASE/TRAIT'].apply(lambda x: replace_acronyms(x, all_acronyms))\n",
    "study_info['MappedExp'] = study_info['MAPPED_TRAIT'].apply(lambda x: replace_acronyms(x, all_acronyms))\n",
    "trait_info['TraitExp'] = trait_info['Disease trait'].apply(lambda x: replace_acronyms(x, all_acronyms))\n",
    "trait_info['MappedExp'] = trait_info['EFO term'].apply(lambda x: replace_acronyms(x, all_acronyms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T19:21:20.964552Z",
     "start_time": "2025-10-03T19:14:39.443612Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1807it [06:41,  4.50it/s]\n"
     ]
    }
   ],
   "source": [
    "# identify the best matches\n",
    "for i, row in tqdm(multi_match.iterrows()):\n",
    "    match_dict[row['Disease trait']] = map_trait(row['Disease trait'], trait_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T19:21:21.405595Z",
     "start_time": "2025-10-03T19:21:21.036551Z"
    }
   },
   "outputs": [],
   "source": [
    "# Make data frame from match_dict\n",
    "match_df = pd.DataFrame(match_dict).T.reset_index()\n",
    "match_df.columns = ['DISEASE/TRAIT', 'MAPPED_TRAIT_CLEAN', 'TRAIT_CODE_CLEAN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T19:21:21.502736Z",
     "start_time": "2025-10-03T19:21:21.475464Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated mappings: 3138\n",
      "Retained mappings: 16314\n"
     ]
    }
   ],
   "source": [
    "clean_study_info = study_info.merge(match_df, on='DISEASE/TRAIT', how='left')\n",
    "print('Updated mappings:', clean_study_info[clean_study_info.MAPPED_TRAIT_URI != clean_study_info.TRAIT_CODE_CLEAN].shape[0])\n",
    "print('Retained mappings:',clean_study_info[clean_study_info.MAPPED_TRAIT_URI == clean_study_info.TRAIT_CODE_CLEAN].shape[0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T19:21:21.905088Z",
     "start_time": "2025-10-03T19:21:21.597969Z"
    }
   },
   "outputs": [],
   "source": [
    "clean_study_info.to_csv(os.path.join(datadir,'Reference_Data',  'cleaned_gwas-catalog-v1.0.3.1-studies-r2025-03-26.tsv.update'), sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CV Association Data Cleaning\n",
    "\n",
    "Associations are filtered to those:\n",
    "* With P-value less than a given threshold\n",
    "* With mapped gene and trait information\n",
    "* Not in intergenic regions\n",
    "* Mapped to a single gene and single trait within a single study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T19:21:22.005296Z",
     "start_time": "2025-10-03T19:21:22.001193Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean_gwas_catalog_data(datafile, outfile, pval_th=5e-8, include_intergenic=False):\n",
    "    \"\"\"Clean the GWAS Catalog data and write to a new file.\n",
    "\n",
    "    Args:\n",
    "        datafile (str): file path for GWAS Catalog data\n",
    "        outfile (str): output file for cleaned data\n",
    "        pval_th (float): p-value threshold for filtering\n",
    "        include_intergenic (bool): whether to include intergenic associations\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    cols= ['DATE', 'PUBMEDID', 'DISEASE/TRAIT', 'MAPPED_GENE', 'SNP_GENE_IDS', 'P-VALUE', 'OR or BETA' ,'MAPPED_TRAIT', \n",
    "           'MAPPED_TRAIT_URI', 'INTERGENIC', 'STUDY ACCESSION', 'SNP_ID_CURRENT', 'INITIAL SAMPLE SIZE', 'GENOTYPING TECHNOLOGY']    \n",
    "    if include_intergenic:\n",
    "        cols = cols + ['UPSTREAM_GENE_ID', 'DOWNSTREAM_GENE_ID', 'UPSTREAM_GENE_DISTANCE', 'DOWNSTREAM_GENE_DISTANCE']\n",
    "    data = pd.read_csv(datafile, sep=\"\\t\", usecols=cols, low_memory=False)\n",
    "    # filter on pval\n",
    "    data = data[data[\"P-VALUE\"] <= pval_th]\n",
    "    # filter on gene and trait present\n",
    "    data = data.dropna(subset=['SNP_GENE_IDS', \"MAPPED_TRAIT_URI\"])\n",
    "    # filter out intergenic\n",
    "    if not include_intergenic:\n",
    "        data = data[data[\"INTERGENIC\"] == 0]\n",
    "    # remove associations with multiple genes\n",
    "    data = data[~data[\"SNP_GENE_IDS\"].str.contains(\",\")]\n",
    "    # remove associations with multiple traits\n",
    "    data = data[~data[\"MAPPED_TRAIT_URI\"].str.contains(\",\")]\n",
    "    # create trait code\n",
    "    data['TRAIT_CODE'] = data['MAPPED_TRAIT_URI'].apply(lambda x: x.split('/')[-1])\n",
    "    # write the cleaned file\n",
    "    data.to_csv(outfile, sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T19:21:30.193218Z",
     "start_time": "2025-10-03T19:21:22.099718Z"
    }
   },
   "outputs": [],
   "source": [
    "clean_gwas_catalog_data(os.path.join(datadir, 'Reference_Data', 'gwas_catalog_Jan29_2025.txt.gz'), \n",
    "                        os.path.join(datadir, 'Reference_Data', 'gwas_catalog_Jan29_2025.txt.cleaned.update'), pval_th=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T19:21:31.421349Z",
     "start_time": "2025-10-03T19:21:30.289480Z"
    }
   },
   "outputs": [],
   "source": [
    "gwas_genes= pd.read_csv(os.path.join(datadir, 'Reference_Data', 'gwas_catalog_Jan29_2025.txt.cleaned.gz'), sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Map identifiers to NCBI Gene IDs (Entrez IDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T19:23:13.482161Z",
     "start_time": "2025-10-03T19:21:31.528630Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query batch 0 - 1000\n",
      "Query batch 1000 - 2000\n",
      "Query batch 2000 - 3000\n",
      "Query batch 3000 - 4000\n",
      "Query batch 4000 - 5000\n",
      "Query batch 5000 - 6000\n",
      "Query batch 6000 - 7000\n",
      "Query batch 7000 - 8000\n",
      "Query batch 8000 - 9000\n",
      "Query batch 9000 - 10000\n",
      "Query batch 10000 - 11000\n",
      "Query batch 11000 - 12000\n",
      "Query batch 12000 - 13000\n",
      "Query batch 13000 - 14000\n",
      "Query batch 14000 - 15000\n",
      "Query batch 15000 - 16000\n",
      "Query batch 16000 - 17000\n",
      "Query batch 17000 - 17133\n"
     ]
    }
   ],
   "source": [
    "# First map from Ensembl\n",
    "ensembl_map, missing = qe.get_latest_ensembl_id(gwas_genes['SNP_GENE_IDS'].unique())\n",
    "ensembl_to_entrez, missing_entrez = gm.convert_node_ids(ensembl_map['to'].values, 'Ensembl', 'Entrez')\n",
    "ensembl_map['Entrez'] = [ensembl_to_entrez[x] if x in ensembl_to_entrez else '' for x in ensembl_map['to']]\n",
    "id_ensembl = gwas_genes.merge(ensembl_map.loc[:, ('from', 'Entrez')], left_on='SNP_GENE_IDS', right_on='from', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T19:26:02.253123Z",
     "start_time": "2025-10-03T19:25:48.148563Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Ids 1108\n",
      "Checking approved symbols\n",
      "Response received\n",
      "Check names 8\n",
      "Previous Ids 8\n",
      "Checking previous symbols\n",
      "Alias Ids 2\n",
      "Searching aliases\n",
      "Searching Entrez\n"
     ]
    }
   ],
   "source": [
    "# Try mapping based on symbols for unsuccessful conversions \n",
    "symbol_map, symbol_missing = qh.perform_hgnc_query(id_ensembl[(id_ensembl['Entrez'].isnull()) | (id_ensembl['Entrez']== '')]['MAPPED_GENE'].unique(), 'Symbol', 'Symbol')\n",
    "symbol_to_entrez, missing = gm.convert_node_ids(list(symbol_map.values()), 'Symbol', 'Entrez')\n",
    "symbol_map = pd.DataFrame(symbol_map.items(), columns=['from', 'to'])\n",
    "symbol_map['Entrez'] = [symbol_to_entrez[x] if x in symbol_to_entrez else '' for x in symbol_map['to']]\n",
    "id_symbol = gwas_genes.iloc[~id_ensembl.index].merge(symbol_map.loc[:, ('from', 'Entrez')], left_on='MAPPED_GENE', right_on='from', how='inner')\n",
    "id_ensembl = id_ensembl[(id_ensembl['Entrez'] != '') & (~id_ensembl['Entrez'].isnull())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T19:26:05.434971Z",
     "start_time": "2025-10-03T19:26:02.389836Z"
    }
   },
   "outputs": [],
   "source": [
    "# Put all together\n",
    "converted_gwas_genes = pd.concat([id_ensembl, id_symbol])\n",
    "converted_gwas_genes = converted_gwas_genes[converted_gwas_genes['Entrez'] != '']\n",
    "converted_gwas_genes.to_csv(os.path.join(datadir, 'Reference_Data', 'gwas_catalog_Jan29_2025.txt.cleaned.entrez.update'), \n",
    "                                sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAVAR - Rare Variant Associations\n",
    "\n",
    "RAVAR Study information can be downloaded from http://www.ravar.bio/#/downloads. Study size and cohort details were manually curated from the associated publications.\n",
    "\n",
    "### Initial study filtering\n",
    "\n",
    "Studies are filtered to:\n",
    "* Exclude reviews and non-genome wide studies\n",
    "* Exclude studies with missing sample size infomation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T19:26:22.307690Z",
     "start_time": "2025-10-03T19:26:22.239313Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total initial studies: 2921\n",
      "- Excluded studies 2864\n",
      "- Missing N: 2864\n"
     ]
    }
   ],
   "source": [
    "# This study information includes manually curated sample size and cohort information\n",
    "rv_study_info = pd.read_csv(os.path.join(datadir, 'Reference_Data','rv_study_info_cleaned_with_manual.tsv'), sep='\\t', index_col=0)\n",
    "print('Total initial studies:', len(rv_study_info))\n",
    "# Exclude reviews and other excluded studies\n",
    "rv_study_info = rv_study_info[(~rv_study_info.COHORT.isin(['Review', 'Exclude'])) &  (rv_study_info['Classification']!='Exclude')]\n",
    "print('- Excluded studies',  rv_study_info.shape[0])\n",
    "# exclude studies with missing sample size information\n",
    "rv_study_info = rv_study_info.dropna(subset=['N'])\n",
    "print('- Missing N:',  len( rv_study_info))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T19:26:23.861651Z",
     "start_time": "2025-10-03T19:26:23.702732Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total initial traits: 2005\n"
     ]
    }
   ],
   "source": [
    "# Load provided trait information\n",
    "rv_trait_info = pd.read_csv(os.path.join(datadir,'Reference_Data', 'trait_allinfo_06112024.txt'), sep='\\t')\n",
    "print('Total initial traits:', len(rv_trait_info))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trait mapping\n",
    "\n",
    "Trait to EFO mappings are updated to harmonize with the GWAS catalog data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T19:26:27.914362Z",
     "start_time": "2025-10-03T19:26:27.896570Z"
    }
   },
   "outputs": [],
   "source": [
    "# expand acronyms in both datasets\n",
    "rv_study_info['Trait'] = rv_study_info['Reported Trait'].apply(lambda z: z.lower())\n",
    "rv_study_info['Mapped'] = rv_study_info['Trait Label'].apply(lambda z: z.lower())\n",
    "rv_study_info['TraitExp'] = rv_study_info['Reported Trait'].apply(lambda x: replace_acronyms(x, all_acronyms).lower())\n",
    "rv_study_info['MappedExp'] = rv_study_info['Trait Label'].apply(lambda x: replace_acronyms(x, all_acronyms).lower())\n",
    "rv_trait_info['Mapped'] = rv_trait_info['Trait Label'].apply(lambda x: replace_acronyms(x, all_acronyms).lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T19:26:29.292278Z",
     "start_time": "2025-10-03T19:26:28.510177Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get GWAS mappings (clean and filtered)\n",
    "clean_study_info = pd.read_csv(os.path.join(datadir, 'Reference_Data', 'cleaned_gwas-catalog-v1.0.3.1-studies-r2025-03-26.tsv.gz'), sep='\\t').dropna(subset='MAPPED_TRAIT_CLEAN')\n",
    "clean_study_info['Trait'] = clean_study_info['DISEASE/TRAIT'].apply(lambda z: z.lower())\n",
    "clean_study_info['Mapped'] = clean_study_info['MAPPED_TRAIT_CLEAN'].apply(lambda z: z.lower())\n",
    "# (original)\n",
    "gwas_trait_info = pd.read_csv(os.path.join(datadir, 'Reference_Data' , 'gwas_catalog_trait-mappings_r2025-03-26.tsv.gz'), sep='\\t')\n",
    "gwas_trait_info['Trait'] = gwas_trait_info['Disease trait'].apply(lambda z: z.lower())\n",
    "gwas_trait_info['Mapped'] = gwas_trait_info['EFO term'].apply(lambda z: z.lower())\n",
    "gwas_trait_info['TraitExp'] = gwas_trait_info['Disease trait'].apply(lambda x: replace_acronyms(x, all_acronyms).lower())\n",
    "gwas_trait_info['MappedExp'] = gwas_trait_info['EFO term'].apply(lambda x: replace_acronyms(x, all_acronyms).lower())\n",
    "gwas_trait_info = gwas_trait_info[gwas_trait_info['Mapped'].isin(clean_study_info['Mapped'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T19:26:38.595916Z",
     "start_time": "2025-10-03T19:26:29.636469Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact trait matches: 342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 342/342 [00:08<00:00, 38.25it/s]\n"
     ]
    }
   ],
   "source": [
    "# Identify exact trait description matches between RAVAR and GWAS Catalog\n",
    "matches = set(gwas_trait_info['Trait']).intersection(set(rv_study_info['Trait']))\n",
    "print('Exact trait matches:', len(matches))\n",
    "# add to match dictionary\n",
    "match_dict = {}\n",
    "for txt in tqdm(matches):\n",
    "    match_dict[txt] = map_trait(txt, gwas_trait_info, usecol='Trait')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T19:26:44.371096Z",
     "start_time": "2025-10-03T19:26:38.742264Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches to MAPPED GWAS traits: 138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 138/138 [00:05<00:00, 24.56it/s]\n"
     ]
    }
   ],
   "source": [
    "# Identify exact matches between RAVAR traits and Mapped GWAS traits\n",
    "matches2 = set(gwas_trait_info['Mapped'].values).intersection(set(rv_study_info[~rv_study_info.Trait.isin(matches)]['Trait'].values))\n",
    "print('Matches to MAPPED GWAS traits:', len(matches2))\n",
    "# add to trait dictionary\n",
    "for txt in tqdm(matches2):\n",
    "     match_dict[txt] = map_trait(txt, gwas_trait_info, usecol='Mapped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T19:26:44.516679Z",
     "start_time": "2025-10-03T19:26:44.514468Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total matches so far: 480\n"
     ]
    }
   ],
   "source": [
    "matched_traits = matches.union(matches2)\n",
    "print('Total matches so far:', len(matched_traits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T19:26:49.640109Z",
     "start_time": "2025-10-03T19:26:44.661521Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches to MAPPED RAVAR traits: 193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 193/193 [00:04<00:00, 38.87it/s]\n"
     ]
    }
   ],
   "source": [
    "# Identify matches between GWAS traits and mapped RAVAR traits\n",
    "matches3 = set(gwas_trait_info['Trait'].values).intersection(set(rv_study_info[~rv_study_info.Trait.isin(matched_traits)]['Mapped'].values))\n",
    "print('Matches to MAPPED RAVAR traits:',len(matches3))\n",
    "map_dict = {}\n",
    "for txt in tqdm(matches3):\n",
    "     map_dict[txt] = map_trait(txt, gwas_trait_info, usecol='Trait')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T19:26:49.786470Z",
     "start_time": "2025-10-03T19:26:49.783694Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total matches so far: 673\n"
     ]
    }
   ],
   "source": [
    "print('Total matches so far:', len(match_dict) + len(map_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T19:26:49.946648Z",
     "start_time": "2025-10-03T19:26:49.930699Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a final dataset of trait matches\n",
    "trait_df = pd.DataFrame(match_dict).T.reset_index()\n",
    "trait_df.columns = ['Trait', 'MAPPED_TRAIT_CLEAN', 'TRAIT_CODE_CLEAN']\n",
    "trait_df['TRAIT_CODE_CLEAN'] = trait_df['TRAIT_CODE_CLEAN'].apply(lambda x: x.split('/')[-1])\n",
    "# And trait mappings\n",
    "mapped_df = pd.DataFrame(map_dict).T.reset_index()\n",
    "mapped_df.columns = ['Mapped', 'MAPPED_TRAIT_CLEAN', 'TRAIT_CODE_CLEAN']\n",
    "mapped_df['TRAIT_CODE_CLEAN'] = mapped_df['TRAIT_CODE_CLEAN'].apply(lambda x: x.split('/')[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final RV study information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T19:26:50.104309Z",
     "start_time": "2025-10-03T19:26:50.095874Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total mapped: 1165\n"
     ]
    }
   ],
   "source": [
    "# Combine results\n",
    "rv_study_info1 = rv_study_info.merge(trait_df, on='Trait', how='inner')\n",
    "rv_study_info2 = rv_study_info[~rv_study_info.rv_idx.isin(rv_study_info1.rv_idx.values)].merge(mapped_df, on='Mapped', how='inner')\n",
    "rv_study_info_mapped = pd.concat([rv_study_info1, rv_study_info2])\n",
    "print('Total mapped:', len(rv_study_info_mapped))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T19:26:50.283174Z",
     "start_time": "2025-10-03T19:26:50.277172Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total remaining: 1699\n"
     ]
    }
   ],
   "source": [
    "# For any traits not included about, retain the original mappings\n",
    "rv_study_info_remaining = rv_study_info[~rv_study_info.rv_idx.isin(rv_study_info_mapped.rv_idx.values)].copy()\n",
    "rv_study_info_remaining['MAPPED_TRAIT_CLEAN'] = rv_study_info_remaining['Mapped']\n",
    "rv_study_info_remaining['TRAIT_CODE_CLEAN'] = rv_study_info_remaining['Trait Ontology id'].apply(lambda x: x.replace(':', '_'))\n",
    "print('Total remaining:', len(rv_study_info_remaining))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T19:26:50.463331Z",
     "start_time": "2025-10-03T19:26:50.460506Z"
    }
   },
   "outputs": [],
   "source": [
    "rv_study_info_out = pd.concat([rv_study_info_mapped, rv_study_info_remaining])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T19:26:50.680663Z",
     "start_time": "2025-10-03T19:26:50.651620Z"
    }
   },
   "outputs": [],
   "source": [
    "rv_study_info_out.to_csv(os.path.join(datadir, 'Reference_Data','rv_study_info_cleaned_with_manual_mapped_Mar28.tsv.update'), sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RV Association Data Cleaning\n",
    "\n",
    "RV Association Data can be downloaded from http://www.ravar.bio/#/downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T19:26:50.879646Z",
     "start_time": "2025-10-03T19:26:50.866678Z"
    }
   },
   "outputs": [],
   "source": [
    "rv_study_info = pd.read_csv(os.path.join(datadir,'Reference_Data', 'rv_study_info_cleaned_with_manual_mapped_Mar28.tsv'),\n",
    "                            sep='\\t', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T19:26:52.173957Z",
     "start_time": "2025-10-03T19:26:51.067487Z"
    }
   },
   "outputs": [],
   "source": [
    "ravar_genes = pd.read_csv(os.path.join(datadir,'Reference_Data' ,'gene_fulltable_06112024.txt.gz'),sep='\\t', \n",
    "                            usecols=['Gene Symbol', 'Ensembl ID', 'Gene Type', 'CHR', 'Location', 'Reported Trait', \n",
    "                                     'Trait Label', 'Trait Ontology id', 'EFO synonym', 'P-value', 'PMID'],\n",
    "                         low_memory=False)\n",
    "#replace '−' with '-'\n",
    "ravar_genes['P-value'] = ravar_genes['P-value'].apply(lambda x: float(x.replace('−','-')) if type(x) == str else float(x))\n",
    "ravar_genes['TRAIT_CODE'] = ravar_genes['Trait Ontology id'].apply(lambda x: x.replace(\":\", \"_\") if type(x) == str else x)\n",
    "ravar_genes['logp'] = -1 * np.log10(ravar_genes['P-value'] + 1e-250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T19:27:15.242821Z",
     "start_time": "2025-10-03T19:27:15.227835Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reported traits 3049\n",
      "Total traits with Ontology 1468\n",
      "Total genes 12850\n",
      "Unique studies 200\n",
      "Min p-value 0.0\n",
      "Max p-value 9.94e-05\n"
     ]
    }
   ],
   "source": [
    "print('Reported traits', len(ravar_genes['Reported Trait'].unique()))\n",
    "print('Total traits with Ontology', len(ravar_genes['Trait Ontology id'].unique()))\n",
    "print('Total genes', len(ravar_genes['Gene Symbol'].unique()))\n",
    "print('Unique studies', len(ravar_genes['PMID'].unique()))\n",
    "print('Min p-value', ravar_genes['P-value'].min())\n",
    "print('Max p-value', ravar_genes['P-value'].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Map identifiers to NCBI Gene IDs (Entrez IDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T19:28:44.614277Z",
     "start_time": "2025-10-03T19:27:31.792738Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query batch 0 - 1000\n",
      "Query batch 1000 - 2000\n",
      "Query batch 2000 - 3000\n",
      "Query batch 3000 - 4000\n",
      "Query batch 4000 - 5000\n",
      "Query batch 5000 - 6000\n",
      "Query batch 6000 - 7000\n",
      "Query batch 7000 - 8000\n",
      "Query batch 8000 - 9000\n",
      "Query batch 9000 - 10000\n",
      "Query batch 10000 - 11000\n",
      "Query batch 11000 - 12000\n",
      "Query batch 12000 - 12850\n"
     ]
    }
   ],
   "source": [
    "ensembl_map, missing = qe.get_latest_ensembl_id(ravar_genes['Ensembl ID'].unique())\n",
    "ensembl_to_entrez, missing_entrez = gm.convert_node_ids(ensembl_map['to'].values, 'Ensembl', 'Entrez')\n",
    "ensembl_map['Entrez'] = [ensembl_to_entrez[x] if x in ensembl_to_entrez else '' for x in ensembl_map['to']]\n",
    "id_ensembl = ravar_genes.merge(ensembl_map.loc[:, ('from', 'Entrez')], left_on='Ensembl ID', right_on='from', how='inner')\n",
    "id_ensembl = id_ensembl[id_ensembl['Entrez'] != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T19:28:49.866093Z",
     "start_time": "2025-10-03T19:28:44.808652Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Ids 61\n",
      "Checking approved symbols\n",
      "Response received\n",
      "Check names 4\n",
      "Previous Ids 2\n",
      "Checking previous symbols\n"
     ]
    }
   ],
   "source": [
    "if len(missing_entrez) > 0:\n",
    "    symbol_map, symbol_missing = qh.perform_hgnc_query(ravar_genes[ravar_genes['Ensembl ID'].isin(missing_entrez)]['Gene Symbol'].unique(), 'Symbol', 'Symbol')\n",
    "    symbol_to_entrez, missing = gm.convert_node_ids(list(symbol_map.values()), 'Symbol', 'Entrez')\n",
    "    symbol_map = pd.DataFrame(symbol_map.items(), columns=['from', 'to'])\n",
    "    symbol_map['Entrez'] = [symbol_to_entrez[x] if x in symbol_to_entrez else '' for x in symbol_map['to']]\n",
    "    id_symbol = ravar_genes.iloc[~id_ensembl.index].merge(symbol_map.loc[:, ('from', 'Entrez')], left_on='Gene Symbol', right_on='from', how='inner')\n",
    "    converted_ravar_genes = pd.concat([id_ensembl, id_symbol])\n",
    "else:\n",
    "    converted_ravar_genes = id_ensembl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T19:28:50.183016Z",
     "start_time": "2025-10-03T19:28:50.062327Z"
    }
   },
   "outputs": [],
   "source": [
    "# add the PCMIDS to the converted data\n",
    "converted_ravar_genes = converted_ravar_genes.merge(ravar_genes.loc[:, ('PMID', 'Ensembl ID', 'Trait Label', 'P-value', 'Reported Trait', 'Location',\n",
    "                        'Gene Symbol')].drop_duplicates(), on=['Gene Symbol', 'Ensembl ID', 'Trait Label', 'P-value', 'Reported Trait', 'Location'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T19:28:51.022313Z",
     "start_time": "2025-10-03T19:28:50.381281Z"
    }
   },
   "outputs": [],
   "source": [
    "converted_ravar_genes.to_csv(os.path.join(datadir,'Reference_Data' ,'gene_fulltable_06112024.txt.entrez.update'), sep='\\t', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (CARVA)",
   "language": "python",
   "name": "carva"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "240.663px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
